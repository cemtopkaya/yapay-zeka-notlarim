## Course Overview

### Course Overview

Hello, everyone. My name is Tom Taulli, and welcome to my course, Practical Application of LLMs, or Large Language Models. As for my background, I'm an IT consultant and author. For businesses and developers, understanding LLMs is absolutely critical. This powerful technology has quickly become a high priority for companies. It's at the same level as cybersecurity and cloud computing. In this course, we are going to get a strong foundation of the key skills and knowledge about LLMs. We'll also have several demos for a tree creator, sentiment analysis app, and LangChain for automation. These will be done using the OpenAI API and Python. Some of the major topics that we'll cover include the following, how LLMs work such as with looking at the transformer model, highlighting the benefits like automation, efficiency, and creativity, as well as the risks, such as hallucinations, bias, and data security, understanding the different types of LLMs, these also include open source projects, learning about how to use prompt engineering, showing how to train LLMs such as with one and few‑shot learning and fine‑tuning, looking at the cutting edge research with LLMs. By the end of this course, you'll have a solid understanding of the fundamentals of LLMs. Before getting this course, you should know how to program in Python and use the Jupyter Notebook. You should also have a general background in machine learning and deep learning. From here, you should feel comfortable diving into topics with courses on intermediate or advanced LLM APIs, prompt engineering, and plugin development. I hope you'll join me on this journey to learn LLMs with this course, Practical Applications of LLMs at Pluralsight.

## Understanding LLMs

### Welcome

Welcome to Practical Application of LLMs, or Large Language Models. My name is Tom Taulli, and I'll be your instructor. We'll be covering the fundamentals of this powerful technology. So then, what is an LLM? Well, to answer this, I'll go to ChatGPT. I won't read the whole thing, but let's take a look at the key points here. An LLM is an AI platform that understands and generates human‑like text. These models are trained on enormous amounts of internet content like Wikipedia and Reddit. The core technology is called the transformer, which is a sophisticated neural network. This is very good at interpreting sequential data and dependencies. This makes LLMs particularly good with language tasks. But in this course, we're going to go a lot deeper into what LLMs are all about. We have four modules. The first one is about understanding LLMs. Here we'll look at the benefits like automation, efficiency, creativity, and the ability to code better. But we'll also look at the downsides. Some of these include hallucinations, troubles with calculations, issues with ethics and fairness, and problems with data security. Yeah, there are a lot of issues. But for the most part, again, LLMs are an extremely powerful technology and are here to stay. We'll also get a high‑level overview of the transformer model. Then we'll look at some of the various types of LLMs like those from OpenAI and Google, as well as open source systems. For the next module, it will be about training LLMs. We'll first see how to use prompt engineering. Then we'll look at how to leverage this for different methods of training, say with one and few‑shot learning. After this, we'll see how to do more traditional training of models such as with large datasets. This will include working with vector databases and word embeddings. We'll then have a module about real‑world applications of LLMs. Here, we'll have two demos. One will be about creating tweets and using sentiment analysis. The next demo will show how to use LangChain, which is an open source system that allows for automating LLM processes. We'll cover dynamic prompts and chaining as well. Finally, we'll have a module about the latest research and advancements. Yes, the LLM category is moving fast. It really can be difficult to keep up with everything. But in this module, we'll take a look at some of the interesting innovations. In this course, I'll have some coding. This will be by using Python. You should have a beginner level of this language. I'll be using version 3.11.3 of Python, as well as Jupyter notebook. So what about me? I'm a self‑taught developer. I got my start in programming in the early 1980s. I learned to code in Basic and then went into other languages like Pascal and C. In the 1990s, I started several companies in the internet space. As for LLMs, I have written a book on the topic. It's called Generative AI: How ChatGPT and Other AI tools Will Revolutionize Business. I've also consulted for various companies about LLMs. We're now done with this introduction. In the next video, we'll take a look at the pros and cons of LLMs.

### Pros and Cons of LLMs

We're going to take a look at the pros and cons of LLMs. So let's start with the benefits of this powerful technology. If anything, the explosive launch of ChatGPT in November 2022 highlighted the incredible capabilities of LLMs. The app took only 2 months to reach a staggering 100 million users. This was the fastest ever, beating out TikTok. What was so amazing to people was that ChatGPT could provide an answer to almost any question and do so with human‑like content. In a sense, it showed the ability to reason and be creative. It could also be funny and sarcastic. Basically, with an LLM like ChatGPT, you don't have to do the typical things for creating an AI project to get good results. It's all in the LLM. Out of the box, you have the capabilities like language translation, code creation, summarization, and text creation. But the LLM does allow for fine‑tuning. This means you can add datasets to it such as for more specific domains. This could be, for example, like a manual for employee benefits. Basically, you can build on top of the LLM. This means you don't have to start from scratch. This is why LLMs are often called foundation models. They are massive and provide lots of capabilities, but you can extend them with fine‑tuning and other approaches. For the most part, LLMs do appear to have a huge impact on society. Some analysts say this is on par with innovations like electricity. If so, LLMs will unleash improvements in productivity and efficiency, allowing for more economic growth. The technology is also likely to impact just about every part of a business. One of the earliest use cases was with marketing and social media campaigns. Then there was the introduction of LLMs to help with code development, as well as helping with cybersecurity. But LLMs can also help streamline processes for legal, HR, procurement, product development, and finance, just to name a few. LLMs are part of something called generative AI. This includes other types of technology such as recreating images or using voice or video. According to economists at Goldman Sachs, generative AI could increase global gross domestic product by 7% or about 7 trillion and add 1.5% in productivity over the next decade. Yet any transformative technology is not without its downsides, and there are several that are fairly large regenerative AI. First of all, LLMs can hallucinate. This means that they can create content that is convincing, but still false or misleading. Why so? LLMs are based on the probabilities of complex relationships among words. So if the underlying data has bad information or bias, this can be reflected in the output. Unfortunately, this can even lead to problems with fairness and discrimination. For example, an LLM may create content assuming that an executive is a male and a nurse is a female. Now, systems like GPT‑4 are making improvements with this. Part of this is due to something called reinforcement learning and human feedback. This is where humans provide evaluations of the results. All in all, it has been a good approach, but there is still a lot that needs to be done. Although one useful strategy is to provide citations for any of the content generated. Security is another issue. A system like ChatGPT will store data up to 30 days, but this may go against tough regulatory requirements such as in industries like health care and financial services. Costs are another issue. LLMs are massive and take huge amounts of compute power to operate. So when using this technology, it may not make sense to use it for certain use cases. Although as the competition heats up in the LLM market, this should help lower the costs. Another challenge with the LLMs is benchmarking. That is, it can be very difficult to compare the capabilities of one model to another. A key reason for this is that the models allow for open‑ended prompts, and they are very, very complex. This makes it tough to have standardized measures, but there are some tools to help out like SuperGLUE, BLEU, ROUGE, Helm and BIG‑Bench. Okay then, we're now finished with this lesson. We've taken a look at the pros and cons. As for the next lesson, we'll show how the transformer model works.

### The Transformer Model

We'll take a look at the transformer model. This is at the core of generative AI for LLMs. However, to better understand this, let's take a look at natural language processing, or NLP. This involves using computers to understand text or spoken words. This not only involves the use of sophisticated AI models, but also computational linguistics, as well as other fields. NLP is actually one of the oldest forms of AI. Keep in mind that it was MIT Professor Joseph Weizenbaum who created the first chatbot in 1965. It was called Eliza. Given the computing resources at the time, the system was fairly basic, often just repeating a user's words that were entered on a keyboard, and the responses were printed on paper. Although for the most part, it was a virtual therapist, and there were some people who thought Eliza was real and were more willing to talk to her than to a person. If you want, you can even check this out on the web here at this URL. Since then, there's certainly been lots of progress with NLP. A big use case of this technology has been with customer support. As deep learning got more sophisticated, this made chatbots better. This was primarily due to recurrent neural networks or RNNs. Yet this technology had limits as it generally processed one word at a time. To allow for more capability, there was the innovation of LSTMs, or long short term memory networks, but even these systems proved limited. Things though would change in a very big way. In 2017. It would be a revolution for NLP and lead to LLMs. The innovation was the transformer, which was introduced in a paper entitled Attention Is all You Need. The authors included researchers at Google and academics at the University of Toronto. The transformer was based on attention mechanisms. Unlike RNNs, it did not process words in order. Instead, it would find the relationships between many words in parallel. Because of this, it allowed for the use of sophisticated machines like GPUs, or graphics processing units. The research paper looked at language translation of English and German and French. The transformer showed standout results. Here's a chart of the transformer. Yes, it's quite complicated. But let's look at the main components. First, here where it says input embedding, words are put into the model and converted to vectors or long lists of numbers. These show numerical relationships between the words. There is then a stack of encoders that iterate across the vectors that use the attention mechanisms. The output of this is then processed in decoders, which are also stacked. This is where word embeddings are turned into new data or words. It's the magic that comes out of ChatGPT. An example of an encoder/decoder model is BART, or bidirectional auto‑regressive transformers. Researchers from Facebook AI created this. But there are also encoder‑only and decoder‑only models. An example of an encoder‑only model is BERT, or bidirectional encoder representations from transformers from Google researchers. Then there's GPT‑3 and GPT‑4 that are decoder‑only models. Now the importance of the attention mechanism is that it allows for understanding the context of language. To get a sense of this, let's take a look at a word that has more than one meaning, bark. Here are two sentences. The dog would always bark at the cat. The bark on the tree was coming off. If we use traditional NLP for this, we would probably not be able to understand the meaning of bark, but a transformer would be able to do so. The reason is that the attention mechanisms will find patterns where words tend to cluster. Dog and cat will likely be associated with bark as when a dog makes a loud sound. But when referring to a tree, then we are looking at another meaning for bark. Something else to keep in mind is that the words generated from a transformer are based on probabilities. It's similar to how the autocomplete works on your smartphone. Let's go to OpenAI's playground and take a look. Down here, let's select Show probabilities, and we'll have those for full spectrum. Then we add this prompt, which is My favorite pet is, and here we get dog. This should not be a surprise. Since OpenAI is trained on huge amounts of internet data, dog would certainly be high on the list of favorite pets. Then we'll click the word, and we get a list of the probabilities of other pets. We have cat, and then we have ham. I'm not sure what ham is referring to, maybe hamster. Sometimes these words can be confusing because they can be cut off. After all, they're tokens. Then we have puppy and dog, and dog is somewhat different because it is capitalized. With an LLM like OpenAI, we can actually change the probabilities such as by adjusting the temperature. This can increase or decrease the randomness or creativity of the responses. It's something that is common when creating apps for LLMs. Okay then so we're now done looking at the transformer model. Next, we'll cover some of the most common LLMs available.

### Types of LLMs

There are various LLMs available. Some are massive, while others are focused on specific use cases. For our purposes, we'll look at some of the major ones. Founded in 2015, OpenAI started as a nonprofit focused on building artificial general intelligence, or AGI. This means that they are developing technologies for superhuman intelligence. At first, they focused on reinforcement learning. But when the transformer model emerged in 2017, OpenAI redirected its efforts at this. But this meant that there was a need for a huge amount of resources. Essentially, OpenAI had to build a highly sophisticated supercomputer for developing LLMs. This is why the company sought out investment from Microsoft. This meant that OpenAI transitioned towards becoming a for profit company. In 2018, the company created GPT or GPT‑1. It was a milestone with LLMs, but the real breakthrough was GPT‑2. No doubt it was a much more powerful model, allowing for summarization, text generation, and translation. Then in 2020, OpenAI announced GPT‑3, and this was another breakthrough for LLMs. GPT‑3 had a massive 175 billion parameters. GPT‑3 also had different versions of the model, which included Davinci, Babbage, Curie, and Ada. These had different levels of performance, speed, and capabilities. Another innovation was that OpenAI turned GPT‑3 into an API. This allowed for third parties to create applications using the LLM. Then came an upgrade, which was GPT‑3.5 Turbo. This was the basis for the hugely popular ChatGPT platform. In March 2023, OpenAI came out with the GPT‑4 system. It was multimodal. That is, it could process text, but also images. GPT‑4 also had much more power such as with reasoning. It was able to rank them on the 10% for the simulated bar exam along with other tough exams. In May 2023, Google launched PaLM 2, or Pathway Language Model. This was at the heart of his API. It was also the foundation for the BART application, a system that's like ChatGPT. Like GPT‑3 and other OpenAI models, there are different versions of PaLM 2. They are Gecko, Otter, Bison, and Unicorn. Keep in mind that PaLM 2 has benefited from advances in optimization. The result is that it does not require as much power and compute. The PaLM 2 LLM can also understand more than 100 languages and has deep reasoning abilities. The system is particularly good with mathematics, as well as science topics. It can also understand and generate code for over 20 programming languages. Before founding Anthropic, the founders were executives at OpenAI. But they believed that there was a better approach to creating LLMs. Basically, the focus would be on creating systems for safe AI. The LLM for Anthropic is called Claude. It has an API and has many use cases, especially for customer service. A key feature of Claude is its 100K context window. This means it's possible to ingest hundreds of pages into the system. For the most part, this is a great feature for large businesses. There are emerging more open source LLMs. One of the leading ones is Dolly, which was created by Databricks. The company is a leader in data management systems. Even before the launch of ChayGPT, the company had over 1000 customers using its LLM technology. Now Dolly is not a huge model, but it is still powerful. Given its small footprint, you can train Dolly on one machine with only 30 minutes of training. And since it's open source, it's possible to create proprietary LLMs. They can also be housed in private clouds, which is very important where security is essential, say, for regulated industries. So then we're now done looking at the different types of LLMs available. We're also finished with this module. For this, we have showed the pros and cons of LLMs, as well as the underlying technology, which is the transformer model. As for the next module, we'll look at the training of LLMs.

## Training LLMs

### Introduction

This module focuses on the different ways to train a model. Granted LLMs are incredibly powerful, but they certainly can do everything. For example, if you are a company and want to have an application for employee benefits, then the LLM will not work because it hasn't been trained on the particular documents for your plan. So there will need to be some customization. And yes, there are various approaches to this. We'll look at some of these in this module. Now one approach is to train a model from scratch. You can use open source software for this and then process the data, and this will be an enormous amount of data. This is critical to make it so that the model will be better able to predict the tokens or words. No doubt, this is expensive and time‑consuming. It could take weeks to train a custom model. Then you'll need to have sophisticated GPUs for the processing. And yes, there will be a need for highly qualified data scientists. For the most part, when it comes to training a model from scratch, this is really not for many organizations. A much less intensive approach is to fine‑tune the base LLM. This is usually much cheaper, but you really should have people with a good background with data science. What's happening with the fine‑tuning is that you are bringing in a dataset and then changing the weights for the model. This should provide better results that are tailored to your needs. It can also mean fewer issues with hallucinations. Next, you can use prompting to train a model. This is a much easier approach, and you don't have to be a data scientist for this. There are different ways for the prompting, whether it be one or few‑shot prompting or learning. However, before doing this, you'll need to have a good foundation with prompt engineering, which we'll take a look at in this module. You can also use something called word embeddings. This allows you to add data to your LLM to determine the spatial distances among tokens or words. This can be very useful for applications like semantic search and recommendation engines. Okay, we're now finished with this introduction for this module. We'll now take a look at prompt engineering.

### Prompt Engineering

We'll take a look at prompt engineering. Even though we have the word engineering in this phrase, this can be kind of misleading. The fact is that the engineering part is really a blend of art and science, and this can be a challenge for developers. There are often no hard and fast rules with prompting as there are with coding. However, there are some best practices and guidelines to consider. So we'll take a look at these. It's important to use the right model. Again, there are often variations. With OpenAI, there is GPT‑3, GPT‑3.5 Turbo, and GPT‑4. All of these have flavors, say like Davinci, Babbage, Curie, and Ada. Of course, each has their own pluses and minuses. But given the open‑ended nature of LLMs when you can prompt it for virtually for anything, it can be tough to figure out what model works and which doesn't. Often the best approach is to try several of the models and see which ones do the best. Yes, it's a lot about trial and error. Also, cost is another important factor. For example, GPT‑4 is certainly powerful, but you may get results you need from a lesser model, which will have a lower price. The prompts will provide an instruction. But to get better results, you usually need to be more specific. One technique is to specify what the LLM should work on. This is common for the use of summarizing information. To demonstrate this, I'll use ChatGPT. For example, here's a prompt, Summarize this information. We have information from Wikipedia about Ada Lovelace, and this is what we get. But we'll likely get better results if we add more detail. We could do something like this. Summarize the text below as bullet points for the most important points. Then we put the text between these three pound signs. This means that the LLM will generally not go off course and consider other information. In other words, we're going to just focus on this and make sure it's summarized. But there are other ways we can add more details to create better prompts. We can specify elements like our goal, length of text, tone, and so on. Suppose we want to create a tweet about this course. For this, I'll use information about the description we have for it, which is between the pound signs here. First, we'll have a goal. Let's say we want to have a discount on the course, that is a promotion. So here's our prompt, Write a tweet about a course called Practical Application of LLMs. It will be to offer a 20% discount. The tone should be upbeat. And this is what you get for the completion. Another way you can provide more details is to indicate the format. You can do this by, say, turning a blog into HTML. Let's do that. Here's the prompt, Write a blog about large language models. It should describe the pros and cons. The blog should be 400 words or so. Write the blog in HTML format. And here's what we get, and we get the HTML format. Pretty cool. When it comes to prompt engineering, your first prompt may not be the right one. But this is very common. A good prompt is often the result of some experimentation and iteration. Now there is much more to prompt engineering than what we've covered so far. And yes, this is a dynamic area. Also in this module, we'll look at some other techniques of prompt engineering to make things more effective. Okay then, we're now finished with this lesson. As for the next one, we'll take a look at prompting techniques such as few‑shot prompting, word embeddings, and plugins.

### Few Shot Prompting, Word Embeddings, and Plugins

We'll look at various ways of how to train an LLM using techniques like few‑shot prompting, word embeddings, and plugins or copilots. First of all, in the lesson about prompt engineering, we were using zero‑shot prompting. This means we had one prompt, and we got one response or completion. Often this is just fine. You'll typically get the response you want. But if not, you could try few‑shot prompting. This is where you try to guide or nudge the model by providing examples. It could be one or more of them. However, you should not go overboard with this. With few‑shot prompting, having two or five or maybe even six will usually be fine. To see how this works, let's go to the OpenAI playground. This is a system where we can try out things on our LLM without having going through the intensive process of coding them up. For example, you can do things like change the temperature and other parameters. So for our purposes, let's come up with a prompt. It says that we want to determine the sentiment of these sentences below. But for the first three, we are guiding the model. We have an example of a negative, positive, and neutral sentence. Then we have two more sentences, but we do not say what the sentiment is. Rather, we'll see if the LLM can figure it out. Let's run this, and here's what we get. And yes, it works. This is a simple example, but essentially, this shows how few‑shot prompting works. In fact, it should not be overcomplicated. With just a few examples, you can get the responses you want. A word embedding is a vector, that is a numeric representation of words or tokens. These shows that complex relationships and how they can help provide context and understanding of the language. Typically to use word embeddings, there are specialized vector databases to handle the processing. Some examples include Pinecone and Redis. Let's see what a word embedding looks like. Here's what I have put together for the embedding for one word. You can see this as a long list, but the LLM will have no problem processing it. To generate these word embeddings, there are various approaches. For example, OpenAI has several models for this. These vary in terms of speed, capabilities, and pricing. With the word embedding, you will use a complex algorithm, like a cosine similarity function, to understand the spatial distances among the word embeddings. This will find those words or tokens that are close together and farther apart. This can be very effective for many use cases. Just some examples include recommendation engines, semantic search. In fact, word embeddings are a modern approach for search engines like Google. Anomaly detection. This can be for cases where you try to detect the cyber threat. And classification. To extend the capabilities of LLMs, there are plugins, say, from OpenAI and Copilot, which are based on Microsoft's technology. Basically, this is a way to bring data into the LLM. This can help with the problem where the knowledge of the LLM is limited as of a certain date or for particular datasets. Let's take an example of a plugin. I'm in ChatGPT. Here you can see some of the available plugins like KAYAK and Expedia. To use these plugins, at least as of this recording, you need to have a paid subscription to ChatGPT. Now to activate the plugin capability, go down here. You'll see three dots and I'll click. I'll go to Settings. Then I'll go to Beta features. From here, you can see the option for plugins. This is where you can turn on a plugin. Now there's another requirement. You have to use GPT‑4. At the top, you can see we have this here. We can click and get the store. Then I'll select the KAYAK plugin and then install it. It's very simple. I'll then go and create a prompt. It's for finding a hotel in New York City. This is what we get. You can see we're accessing the KAYAK plugin, and then we get some options for hotels. We even get some thumbnails. We can click these to learn more. So yes, plugins are very powerful. For the most part, they can be a great way to extend the performance of LLMs. Okay, we're now at the end of this lesson. We've taken a look at few‑shot prompting, word embeddings, and plugins. Next, we'll show how to use fine‑tuning with LLMs.

### Fine Tuning an LLM

We'll take a look at fine‑tuning of LLMs. This can help provide much more customization for an application. A major benefit of fine‑tuning is that you can go beyond the fixed limits for the amount of data of a prompt. And yes, this should also provide better quality responses, say with fewer hallucinations and more personalization. Keep in mind that the fine‑tuning will change the weights of the model, so you have to be careful. But in a way, fine‑tuning an LLM is similar to a typical AI project. For the most part, the process can be complicated, requiring people who understand data science. First of all, you will need to get relevant datasets. Usually there will need to be wrangling because the information will have issues. Some of these may be gaps, outliers, and bias. Cleaning up the data can be a tough process and expensive and does require some expertise. Now if you have a dataset that is, say, from Wikipedia or another common source from the internet, then this will probably be a bad candidate for your fine‑tuning. You'll probably get better results by using the LLM with zero‑shot prompting. Instead, the dataset should be unique. This could be data from a customer relationship management system, say with emails or chats, or it could be corporate documents. But when you have the right dataset, you must make sure you have the rights to use it. You should also make sure it is scrubbed for personally identifiable information. These can be tricky issues, and you may need to get some support from the legal department. According to OpenAI, there are two major use cases for fine‑tuning of an LLM. First, it can be very effective for classification, that is putting data into logical groups. This can be useful for applications like spam detection and sentiment analysis. Another use case is conditional generation. This is a way of describing tasks like summarizing, entity extraction, and paraphrasing or rephrasing. Finally with an AI project or a fine‑tuning project, you'll need to be mindful of the cost. This is because many models are based on the number of tokens used. So ahead of time, you really need to get some good estimates. For OpenAI, it allows for fine‑tuning of its LLMs through its API. It is also structured in JSON according to the following. That is there is a prompt and completion format, you can use various types of file formats for this, and you can also use feeds from other APIs or databases. According to OpenAI, your dataset should have at least 100 or more items. Although to be realistic, it probably should be in the thousands so as to get better results. OpenAI does have some tools that you can use in the terminal or Jupyter Notebook that will convert your dataset into a form that the model can process. Then there is a system that can fine‑tune the model. This process can take a few minutes to well over an hour. It just depends on the size of your model and how many people are using the service at the same time. you can also divide the dataset into a training set and validation set as you would with the traditional AI process. This is recommended because the results should be much better. After your model has been fine‑tuned, you'll get a model identifier. You'll place this in your API for access to it. This will also be available on the OpenAI playground. Here you can see some examples that I have. Now even though the fine‑tuning process has gotten easier, this is still intensive and does require expertise. Besides, LLMs are still fairly new. So fine‑tuning is still evolving. There are other issues to keep in mind, such as the breaking of models and model drift. This is where the underlying datasets essentially go stale. Okay then, we're now at the end of this lesson. We've taken a look at the fine‑tuning process for LLMs. We're also finished with this module. As for the next one, we'll take a look at real‑world applications of LLMs.

## Real-world Applications of LLMs

### Introduction

Welcome to our module called Real‑world Applications of LLMs. We'll have two demos. One will be about creating an app for tweets and sentiment analysis. Then we'll have one about LangChain, which is a framework for developing LLM applications. For these demos, we'll be using the OpenAI API. So what we'll do now in this clip is show how to set this up. You'll need to create an online account with OpenAI. Here's the URL for it. The process is like any other signup process. You can have your own email and password credentials, or you can use Google, Microsoft, or Apple. The account is free to set up and you get a $5 credit for the first 3 months. After this, you have to start to pay for the API. Generally, the fee for the service is based on the number of tokens used. Here's the pricing for GPT‑4. So it's 3 cents per 1000 tokens for when you're inputting text for the 8K context. This is for a model that allows for 8000 tokens. Here at this URL, you can get the complete pricing for all the models for OpenAI, such as for GPT‑3, GPT‑3.5 Turbo, and so on. Okay then, so what is a token? Well, it can be a word or a part of a word. This is the way that the LLM is fed text and how it is processed. A way to understand this is to go to the tokenizer from OpenAI. You can enter the text, and it will show the number of tokens. Here's the URL for it. Now I've cut and paste some information about quantum mechanics, and this is what the tokenizer gives us. This shows that we have 54 tokens. The first word, quantum, is composed of two tokens. Also a token will include a space, such as here with mechanics. Punctuation can be a token too. This is the case with the period. And what about emojis? Well, this could actually be several tokens. A rule of thumb is that 1000 tokens will be about 750 words or so. For the API, OpenAI has lots of resources available. There are tutorials, examples, documentation, and, of course, the API reference. But to create an API key, let's go up here where it says Personal and select API keys. To create one, press this button. You can then copy it by pressing this. And make sure that this is kept safe. If you lose the key, then you'll have to create a new one. There are some other useful sections in this webpage. You can check out the usage for your API to see how much you are spending. You can also place usage limits and add or delete members. With our API key, we can create our own application to access the OpenAI LLM. We'll create a very simple one. We'll go to Jupyter Notebook for this. First of all, we need to use the OpenAI library. We'll install this using the pip install command. We specify upgrades as there are frequent updates. Then we import the OpenAI API. We'll also do the same for OS. This gives us access to our operating system for our computer. Why use this? This is to provide security for our API key. If someone gets a hold of it, they can rack up bills in our account. So we'll look at a way to protect our key. We'll do this by creating an environment variable. This is a function that has the API key as the parameter. We then set it up using this code. However, after you do this, make sure you delete the API key value here. So when you upload it, say to GitHub, there won't be any access. Okay, we'll now make a call to the OpenAI API. And there are two approaches for this, but we'll look at the one for chats. This is for models like GPT‑3.5 Turbo, which is what powers ChatGPT and GPT‑4. We set it to the variable response. Then we have a method called OpenAI, and we set this for chat completion and create. We then specify the parameters for the model. We use the GPT‑3.5 Turbo model. This is fairly powerful, but a lot cheaper than GPT‑4. Then we have a list called messages. This has a structure where there is a role and content. There are different types of roles. One is for the system. This essentially sets up the context for the prompt. Here we say that the AI is a generative AI expert. Then we have the role of the user. This is us, the person who enters the prompt. The prompt we have is to write a one‑paragraph description of generative AI. I'll run this by printing out the response variable, and here's what we get. So we get a lot of details. We get details about what has been created like the model and the object description. Then there is the response, which is under choices and message. Assistant is another role. This is the response from the LLM. Here you can see that it has written a definition for generative AI, and it's pretty good. We'll also have other details. Finish\_reason is why the model stopped. In this case, it was because the response was answer fully. Then we have usage stats, such as the number of tokens for the prompt and completion. So yes, we've created our first call for an LLM and got a completion. And yes, we're finished with this introduction. We've looked at the setup and calling for an OpenAI API. Next, we'll take a look at a demo for the creation of tweets and sentiment analysis.

### Demo: Tweets and Sentiment Analysis

We'll have a demo for using the OpenAI API. This will include two tasks. One will be to create tweets, and the other will be for sentiment analysis. Let's get started. We're in Jupyter Notebook, and we have this set up here to access the API where we have the OpenAI and OS libraries. Then we set the API as the environment variable. The API code we'll use to generate tweets will be similar to what we've seen so far. However, what we're going to do is start adding to the prompt. We start off with creating a function. This is called create\_tweet, and it has two parameters, which include the topic for the tweet and the length or maximum length in tokens. We create a variable for the prompt, and we use this to make it so we can have multiple lines. We dynamically change the prompt by including the parameter topic in brackets. The topic will be inserted here. Next we call the OpenAI LLM and use the GPT‑3.5 Turbo model. For the system role, we say that the LLM is an expert at writing tweets. Then we set the user content for prompt. This is what we'll pass to the LLM. Max\_tokens is one of the built‑in parameters for the API. Yes, this will make it so it will not exceed this number, although this can sometimes be half. Then we return the tweet. But instead of using the whole completion, we only want to specify the tweet itself. To do this, we have to map through the JSON. This is at index zero. And then for choices, we specify message and content. We then have the code to call the function. First, we have an intro message. After this, we have input statements for the topic of the tweet and the length. We convert this to an integer. Then we call the function, which we call tweet and then print it out. Let's try this and run it. The topic we use for this is generative AI, and the length is 100 tokens. This is what we get, and it's pretty good. We even have some hashtags. But the use of the max tokens is probably not the best approach. After all, words and tokens are not the same. Besides, Twitter does have a character limit. So let's change the code for this. For the limit, this is at 250. Then in the prompt, we add a sentence to have this character limit specified. I'll run it and we'll put generative ai for the topic, and we'll have 200 here for the character limit. And it looks good. Another parameter we can add is temperature. This essentially increases the randomness or creativity of the content generated. The default is 1. If you want lower amounts of randomness, then you want to go below 1, and the lowest you can go is 0. If you want more randomness, you can go up to 2. So what is better? Well, there are no clear‑cut rules for this. Rather, as is often the case when working with LLMs, this is really about trial and error. However, high levels of temperature can cause some weird results, so let's try this here. We'll set temperature for 2, and we'll run it with the same information. And you can see it's something that definitely won't work for us. It actually looks like it's gone haywire. Basically, when it comes to using the API, you can see we are doing different types of coding. On the one hand, we can change the parameters like temperature, and then we can make adjustments to the prompt. The result is that we can create some very interesting generative AI applications using the LLM. Now something you can do with an LLM is create test data. This can certainly be useful for development. We'll do this by adjusting our prompt for the program with this. We want it to create five tweets for us, and we should have at least one tweet for sentiment of positive, negative, and neutral. I'll run it with the same information, and here's what we get. So we got the five tweets, and it does span different types of sentiment. With this data, I can then create our sentiment analysis program. Again, we have the same structure. First, we have a function that is called get\_sentiment with the parameter of tweet we want to determine the sentiment for. Then we set up the prompt. We want the LLM to determine if the tweet is positive, negative, or neutral. And then we say this is for a tweet that's delimited with three pound signs, and we have the parameter here. We only made one modification for calling the OpenAI LLM. It's for the system message. We're saying that the LLM is an expert in determining sentiment of the text. For the code here, we have an intro message and an input for the tweet. Next we call the function and print out the response. Let's try it out. I'll enter one of the tweets that we got from our previous program, and this is what we get. And yes, it's correct with the sentiment. But we can certainly do more. For example, how about using an emoji? That we can certainly do. We do this in the prompt by adding this, Also include an emoji. I'll run it and input the tweet, and we get the correct response including an emoji. As you can see, with not a lot of programming, we can do a lot of powerful tasks. We can access an LLM for generative AI. But we can also do things like add emojis, which would have taken some considerable coding if we did this the traditional way. Okay, we're now finished with this demo. We've seen how to put together a tweet creator and sentiment app. Next, we'll look at how to use LangChain.

### Demo: LangChain

We'll show how LangChain works. This is an open‑source framework for developing LLM applications. It has a variety of powerful features that make the code more compact and reusable. We'll have a demo on LangChain by using the OpenAI API. So let's get started. We first need to install the library for LangChain. We do this by using the pip install command. Then we do the typical setup for the OpenAI system. We import the OpenAI library and OS. Then we create the environment variable for the API key. Next, we'll do the set up for LangChain. We'll be using the model that allows for chat completions for OpenAI. We'll do an import for LangChain for this. Then we create the structure or schema for LangChain. This is for the different types of roles. AIMessage is what the AI responds to us with. It's the assistant role. Then the HumanMessage is the user role. It's what we prompt. Then the SystemMessage will set up the system for the context of the prompt. We'll have an import for the chat model for OpenAI. Then we'll set up the model and create a prompt. We have a variable called response. This calls ChatOpenAI. The model we're using here is GPT‑3.5 Turbo, and we set a couple of parameters for it. We have temperatures set to 1 and max\_tokens at 100. Then we set up the messages for the prompt. We specify a system message for the message, and we say we are an expert in data science. Then for HumanMessage, we have a prompt to write a paragraph about large language models. We print this out, and we get the response. True this does exactly what the OpenAI does and what we've seen before. However, by doing some of the setup, we have created an object that allows for a more compact and intuitive way to call the LLM. But let's try something else that's a part of LangChain. We'll use a LangChain function called PromptTemplate. What this does is allow us to use dynamic prompts. For example, we can include parameters in them. Here's the code for it. We import the PromptTemplate library. Then we create an object for the LLM we want. This time we'll use GPT‑3. Keep in mind that LangChain allows us to use many LLMs, and they do not have to be from OpenAI. For our LLM object, we're using the text‑davinci‑003 version. Then we set temperature to 0.7 and max\_tokens to 100. We then create a variable called template. For the system, we say we are an expert coder, and the instruction is to write a program for a certain language and to carry out a task. By putting both language and task in brackets, this means we'll include the user‑supplied values for these to make the prompt dynamic. We create a variable called prompt and use the PromptTemplate method. We have two parts to this. First, we have the input variables. We can have one or more of them. Here we have one for the language and the for task. We then set the template to our own variable we created. That is template. Then we have a variable called response, and we use the llm object. We use prompt format to get the output we want. We specify that we want code created for Python, and the function is to calculate the average of two numbers. Let's run it, and this is what we get. Again, we have some setup here with the LangChain. But when it comes to using our LLM object on our code, it is very simple and compact. We just have a couple of parameters. Finally, we'll create a chain using LangChain. It will be a simple sequential chain. Essentially, we'll have two main components. One will use an LLM to create a response. This will then be used as the input for the next component we'll call. Yes, we are chaining together two LLM calls. Let's see how this works. We create the object for our first LLM, which we'll call llm1. This is Davinci 003 model, and we set the parameters. Then we have prompt1 and use PromptTemplate for it. There is a description for the prompt variable, and then we have the template. We say that the AI is a great mathematician. The instruction is to write an equation that is according to the description that we provide. We then create a chain called chain1, which uses LLMChain, and we set llm to llm1 and prompt to prompt1. Now let's create the second component in the chain. We'll have chain2, which has the same information as the prior model used. Next, we create prompt2 and use PromptTemplate for this. Our input variable is equation. In other words, this is taking in an equation that was created from the first chain. We then have the instruction to provide an explanation for this equation. We'll also create a chain object for this called chain2 that uses LLM chain. We'll set llm to llm2 and prompt to prompt2. We have the simple sequential\_chain method and set it to sequential\_chain. This specifies the chains, which are chain1 and chain2. We also have verbose set to true. This means that the AI will provide a description of the process that we're going to go through. Then we execute this by using sequential\_chain.run, and the variable response will be set with the output of the chains. For this, we want the AI to create the Pythagorean Theroem. Let's run it, and this is what we get. This is certainly powerful. In other words, we're writing a sophisticated workflow that uses data from one chain to the other chain. This is also done in a very intuitive way. Okay then, we're finished with this video. We've seen how to use LangChain for development with the OpenAI API. We're also finished with this module. As for the next one, we'll look at the latest research and advancements for LLMs.

## Latest Research and Advancements

### Introduction

In this module, I'll take a look at some of the key areas for research and advancements. This will be divided into two parts. The first one will be about the cutting edge innovations with LLMs. We'll see some of the most advanced research. These will include the improvements such as with optimizations, but also capabilities for mitigating the problems such as with hallucinations. There will also be a look at the areas like fine‑tuning of models. Interestingly enough, a major area of research is to make models smaller without sacrificing the quality of the results. And we'll also look at emerging areas like multimodal LLMs. As for the next part of this module, we'll focus on artificial general intelligence, or AGI. This is where the models take on human like capabilities, if not super human capabilities. We'll take a look at some of the research with this regarding models like GPT‑4. We'll also look at the future of LLMs, for example what comes after the transformer, where we use technology like quantum computing. Okay then, so let's get started.

### Innovations

We'll take a look at some of the innovations with LLMs, and there are many. In fact, the pace of change has been very quick. It seems like every day there's something new. And yes, it is difficult to keep up with everything. It can really be daunting. But with innovations, there are some areas that are critically important and really stand out. For example, when it comes to research, there is a major focus on improving the performance of the models. True this is about often improved accuracy. This has certainly been seen with the progress of OpenAI's GPT‑3 to GPT‑4 progression. They have shown great strides with improving reasoning, as well as mitigating hallucinations and bias. The same is the case with other models, whether it be Google's PaLM 2 or Meta's Llama. With the significant investments in LLMs, we'll definitely see further progress. And yes, it'll probably be an accelerated pace. Let's first take a look at the case with hallucinations. An interesting way to help deal with these is the use of knowledge retrieval. This is where the LLM is bolstered with access to other datasets, such as those that are more domain‑specific. Google has been focused on this research for some time. In 2020, the company published a paper about retrieval augmented language model pretraining, or REALM. What this showed is that this could have better training with unsupervised data for millions of documents. Also for this system, when the user creates a prompt, there is a neural retriever that accesses the right datasets. Other companies have definitely added to the great research. For example, AI21 Labs has created its own variation called in‑context retrieval augmented language modeling. This allows for easier implementation of knowledge retrieval. In fact, this type of technology has been useful in implementing LLM capabilities into search engines like Bing, ChatGPT, and Google's Bard. But there are other important areas where we are seeing improvements with the training of LLMs. These new approaches are about finding ways to help deal with the process. After all, it can be time‑consuming and expensive when you have to deal with advanced and costly GPUs. There's also been advances with optimization for handling the huge file sizes. One technique that has proven to be useful is parameter‑efficient fine‑tuning, or PEFT. This is actually a grouping of different techniques like LoRA, prefix tuning, p‑tuning, and prompt tuning. All these are very sophisticated and backed up by academic research. Basically, with PEFT, you'll fine‑tune a small number of extra weights. Then you'll freeze the rest of them for the LLM. A key advantage to this is that you will not have catastrophic forgetting. This is where the model will have gaps in its memory and give off bad results. But this is not a problem with PEFT because you still have the original weights. Basically, this works quite well when you have small amounts of unique data. It also means that the resulting model will be much smaller yet still powerful and useful. There's much innovation happening on the infrastructure for LLMs. We've already seen how Google has been working with this with PaLM 2. It has fewer parameters, but it is still more powerful than the last generation model. DeepMind, which Google owns, has also been making advancements with optimization. This is set forth in a paper entitled Training Compute Optimal Large Language Models. It notes that the existing LLMs are not adequately trained. This is primarily because of the massive data sizes. So DeepMind has been building its own models, which have much fewer parameters. These are known as sparse or dense models. One of them is called Chinchilla, which has 70 billion parameters and is trained on about 1.3 trillion tokens. Based on various benchmarks, it has shown strong results. They are merging many different techniques to improve optimization. For example, there is shared data parallelism. With this, you are splitting the state of a model across different GPUs. What this means is that as you scale up, you can reduce the GPU memory usage. Another area of innovation has been to deal with something called quadratic scaling. This has to do with the context window. This is how large the prompt can be. For GPT‑4, it's up to 32,000 tokens as of this recording. But as the context windows get bigger and bigger, it will run into the issue of quadratic scaling. That is, as you add to the prompt, the requirements for compute and resources will increase at a quadratic rate. This is due to the complexities of the self‑attention model for the transformer. Quadratic scaling can quickly expand a model and the cost. It also makes it very difficult for open‑source LLMs, which often have less sophisticated infrastructures. But there has been some interesting research with quadratic scaling. It's called FlashAttention. Stanford researchers came up with this and wrote about it in a paper entitled FlashAttention: Fast and Memory‑Efficient Exact Attention with IO‑Awareness. Unlike traditional approaches, FlashAttention has not only been able to be more efficient with quadratic scaling, but also to not sacrifice the quality of the model. It's definitely complex. But generally, it's about taking a more optimal approach with the infrastructure, that is focusing on the SRAM memory for a GPU. Another important trend with LLMs is the use of multimodal capabilities. These models have become known as MLLMs, or multimodal large language models. While a traditional LLM is about text, an MLLM allows for images, audio, and even video. These are still very much in the early stages, but there is a lot of progress being made. Basically with MLLMs, there will be incredible powerful applications. Okay then, we're finished with this clip. Again, there is much happening with the development of LLM technology, but we have focused on some of the areas where there have been improvements in training and cost optimization. Next, we'll take a look at artificial general intelligence, or AGI.

### Artificial General Intelligence

We'll take a look at artificial general intelligence, or AGI. This is the huge vision of some of the most advanced AI companies like OpenAI and DeepMind. Now there's a lot of debate about when we'll reach AGI. In fact, some researchers actually think it is not possible. Yet the advances in LLMs do highlight that it is reasonable that we'll see the emergence of AGI at some point. But as for timing, this is likely to take a while. The chief AI scientist at Meta, Yann LeCun, has a clear perspective on this. Here's one of his tweets. Before we reach human‑level AI, we will have to reach cat‑level and dog‑level AI. We're nowhere near that. We're still missing something big, LLM's linguistic abilities notwithstanding. A house cat has way more common sense and understanding of the world than any LLM. Then there's Rodney Brooks. He's the co‑founder of iRobot. He thinks that AGI will not be a reality until at least the year 2300. Now when it comes to AGI, the irony is that one of the challenges is actually to measure what really is human intelligence. This is something that researchers have struggled with for decades, although it was Alan Turing who came up with an interesting construct for this in a paper he published in 1950, which was entitled Computing Machinery and Intelligence. Turing was a pioneer in developing the core concepts of modern computers, but he was also someone who spent much time thinking about machines that could think. In his paper, he said that defining intelligence was not possible. So what to do? He came up with a thought experiment. He called it the imitation game, although it would ultimately become known as the Turing test. Here's how it works. A questioner will engage in a Q and A. In one room, there is a human in another room, there is a computer, but the questioner does not know the identity of the two. For the Turing test to be passed, the questioner must get to the point that they cannot tell the difference between the two. Alan Turing thought that this would happen by the year 2000. But of course, this proved to be too optimistic. The Turing test is far from perfect and has come under scrutiny. For example, the questioner can find ways to trick an AI system to get a sense that it is an AI. This is actually something that some Turing tests have done without a lens. In fact, AI21 Labs created an experiment for this. It built an application that simulated the Turing test by using an LLM for the AI. It involved sessions for 2 minutes of conversations. What AI21 found was that humans were still pretty good at guessing who was human and a computer. The best performance was in France where the guess rate was 71.3%. The lowest was in India at 63.5%. If anything, there will probably need to be new gauges for measuring AGI. After all, approaches like the Turing test are text‑based, but LLMs are becoming multimodal. So how do you measure something like the visual of someone talking? This will certainly not be easy to do. Although some of the new metrics that are merging are fairly interesting. Take one created by AI expert Gary Marcus. His approach, which is called the Marcus test, says that if the AI watches an episode of The Simpsons and knows when to laugh, then the system has reached AGI. Or there's the Wozniak test or what has become known as the caffeine test. This is from Steve Wozniak, the co‑founder of Apple. He says that we have reached AGI when a system can walk into a stranger's house and make a cup of coffee. A paper from Microsoft researchers and academics evaluated the AGI for GPT‑4. The paper was entitled Sparks of Artificial General Intelligence: Early experiments with GPT‑4. They found that the model did exhibit some levels of human intelligence. It showed how could solve novel and difficult tasks with law, psychology, vision, coding, and mathematics. The authors actually said that GPT‑4 was an early version of AGI. Now when it comes to LLMs, there is likely to be major breakthroughs with the underlying models. If anything, the transformer may be somewhat temporary. This is actually the hope of Sam Altman, who is the co‑founder and CEO of OpenAI. He has said the following. Oh, I feel bad saying this. I doubt we'll still be using the transformers in 5 years. I hope we're not. I hope we'll find something way better. But the transformers obviously have been remarkable. So I think it's important to always look for where I'm going to find the next totally new paradigm. But I think that's the way to make predictions. Don't pay attention to the AI for everything. Can I see something working, and can I see how it predictably gets better? And then, of course, leave room open for you can't plan for the greatness, but sometimes the research breakthrough happens. Then what may replace the transformer? Well, this is obviously very speculative, yet there are some interesting possibilities. One is the use of quantum computing. As the name implies, this is where a computer system is based on the principles of quantum mechanics. The quantum is the smallest entity of a physical property like electrons, neutrinos, and photons. But being at this level, the computing is not done with the traditional binary 0s and 1s, but is instead done with cubits, which include a superposition of all possible states. The result is that you can process complex tasks much quicker, but there's also much less energy use. So yes, it could be revolutionary for LLMs and other sophisticated AI systems. But all this is still in the early stages, although the good news is that there is considerable research with quantum computing and how it relates to AI Okay then, this brings us to the end of this clip. We've seen some of the interesting developments of AGI and LLMs along with a look at the future, and we're also finished with this course. We certainly covered a lot. We've gotten a good understanding of the fundamentals of this powerful technology like transformer models. We've also looked at the use cases and risks. Then we have reviewed ways to train the models, whether it with zero and few‑shot learning and fine‑tuning. Then we have shown how the technology works by doing some demos. LLMs are an exciting space with many opportunities. So thanks for taking this course and good luck on your journey.
